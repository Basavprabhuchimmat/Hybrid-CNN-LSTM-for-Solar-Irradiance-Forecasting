{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b3ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#CNN Regression model for solar irradiance nowcasting from IR sky images.\n",
    "class SolarCNNRegression(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, input_channels=3, num_classes=1):\n",
    "        super(SolarCNNRegression, self).__init__()\n",
    "\n",
    "        # Feature extraction layers (Convolutional backbone)\n",
    "        self.features = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # 240x320 -> 120x160\n",
    "\n",
    "            # Second conv block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # 120x160 -> 60x80\n",
    "\n",
    "            # Third conv block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # 60x80 -> 30x40\n",
    "\n",
    "            # Fourth conv block\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # 30x40 -> 15x20\n",
    "\n",
    "            # Fifth conv block\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling\n",
    "        )\n",
    "\n",
    "        # Regression head for irradiance prediction\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)  # Output: solar irradiance value\n",
    "        )\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.features(x)\n",
    "\n",
    "        # Regression prediction\n",
    "        irradiance = self.regressor(features)\n",
    "\n",
    "        return irradiance\n",
    "\n",
    "#Extract features for LSTM input\n",
    "    def get_features(self, x):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = self.features(x)\n",
    "            return features.flatten(1)  # Flatten for sequence input\n",
    "\n",
    "\n",
    "class SolarCNNWithFeatureExtraction(SolarCNNRegression):\n",
    "\n",
    "\n",
    "    def __init__(self, input_channels=3, feature_dim=512):\n",
    "        super().__init__(input_channels, 1)\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        # Feature projection layer for LSTM input\n",
    "        self.feature_projector = nn.Sequential(\n",
    "            nn.Linear(512, feature_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        # Extract convolutional features\n",
    "        conv_features = self.features(x)\n",
    "        flattened_features = conv_features.flatten(1)\n",
    "\n",
    "        # Get irradiance prediction\n",
    "        irradiance = self.regressor[-3:](\n",
    "            self.regressor[:-3](flattened_features)\n",
    "        )\n",
    "\n",
    "        if return_features:\n",
    "            # Project features for LSTM input\n",
    "            projected_features = self.feature_projector(flattened_features)\n",
    "            return irradiance, projected_features\n",
    "\n",
    "        return irradiance\n",
    "\n",
    "\n",
    "# Legacy model for backward compatibility\n",
    "class SolarCNN(SolarCNNRegression):\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import json\n",
    "import cv2  \n",
    "\n",
    "class MultiDayDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, irradiance_values, target_size=(240, 320)):\n",
    "        self.image_paths = image_paths\n",
    "        self.irradiance_values = irradiance_values\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Could not read image: {img_path}\")\n",
    "\n",
    "        # Resize to target CNN input size\n",
    "        img = cv2.resize(img, self.target_size)\n",
    "\n",
    "        # Convert BGR → RGB and normalize\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_tensor = torch.tensor(img).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        irradiance = torch.tensor(self.irradiance_values[idx], dtype=torch.float32)\n",
    "        return img_tensor, irradiance\n",
    "\n",
    "\n",
    "def get_multi_day_dataset(image_dirs, irradiance_files):\n",
    "    image_paths = []\n",
    "    irradiance_values = []\n",
    "\n",
    "    for img_dir, irr_file in zip(image_dirs, irradiance_files):\n",
    "        # Load irradiance data\n",
    "        if irr_file.endswith('.csv'):\n",
    "            df = pd.read_csv(irr_file)\n",
    "            values = df.iloc[:, 1].values.astype(np.float32)\n",
    "        else:\n",
    "            values = np.loadtxt(irr_file, delimiter=',')[:, 1].astype(np.float32)\n",
    "\n",
    "        files = sorted([f for f in os.listdir(img_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        files = files[:len(values)]  # Ensure matching length\n",
    "\n",
    "        image_paths.extend([os.path.join(img_dir, f) for f in files])\n",
    "        irradiance_values.extend(values[:len(files)])\n",
    "\n",
    "    return MultiDayDataset(image_paths, irradiance_values)\n",
    "\n",
    "class CNNTrainer:\n",
    "    \"\"\"\n",
    "    Trainer class for CNN nowcasting model following paper methodology\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_type='standard', config=None):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Default configuration\n",
    "        self.config = {\n",
    "            'learning_rate': 1e-4,\n",
    "            'batch_size': 32,\n",
    "            'num_epochs': 50,\n",
    "            'weight_decay': 1e-4,\n",
    "            'scheduler_patience': 10,\n",
    "            'early_stopping_patience': 15,\n",
    "            'save_dir': 'models',\n",
    "            'log_dir': 'logs'\n",
    "        }\n",
    "\n",
    "        if config:\n",
    "            self.config.update(config)\n",
    "\n",
    "        # Create directories\n",
    "        os.makedirs(self.config['save_dir'], exist_ok=True)\n",
    "        os.makedirs(self.config['log_dir'], exist_ok=True)\n",
    "\n",
    "        # Initialize model\n",
    "        if model_type == 'with_features':\n",
    "            self.model = SolarCNNWithFeatureExtraction().to(self.device)\n",
    "        else:\n",
    "            self.model = SolarCNNRegression().to(self.device)\n",
    "\n",
    "        # Initialize optimizer and scheduler\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config['learning_rate'],\n",
    "            weight_decay=self.config['weight_decay']\n",
    "        )\n",
    "\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='min',\n",
    "            patience=self.config['scheduler_patience'],\n",
    "            factor=0.5,\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # Training history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        with tqdm(train_loader, desc=\"Training\") as pbar:\n",
    "            for batch_idx, (images, targets) in enumerate(pbar):\n",
    "                images, targets = images.to(self.device), targets.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs.squeeze(), targets)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        return total_loss / num_batches\n",
    "\n",
    "    def validate_epoch(self, val_loader):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        predictions = []\n",
    "        targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with tqdm(val_loader, desc=\"Validation\") as pbar:\n",
    "                for images, batch_targets in pbar:\n",
    "                    images, batch_targets = images.to(self.device), batch_targets.to(self.device)\n",
    "\n",
    "                    outputs = self.model(images)\n",
    "                    loss = self.criterion(outputs.squeeze(), batch_targets)\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "                    targets.extend(batch_targets.cpu().numpy())\n",
    "\n",
    "                    pbar.set_postfix({'Val Loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "\n",
    "        # Calculate additional metrics\n",
    "        predictions = np.array(predictions)\n",
    "        targets = np.array(targets)\n",
    "\n",
    "        mse = mean_squared_error(targets, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(targets, predictions)\n",
    "\n",
    "        return avg_loss, rmse, mae, predictions, targets\n",
    "\n",
    "    def train(self, train_loader, val_loader=None):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        print(f\"Starting training for {self.config['num_epochs']} epochs...\")\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "\n",
    "        for epoch in range(self.config['num_epochs']):\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.config['num_epochs']}\")\n",
    "\n",
    "            # Training\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            self.train_losses.append(train_loss)\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "            # Validation\n",
    "            if val_loader is not None:\n",
    "                val_loss, rmse, mae, predictions, targets = self.validate_epoch(val_loader)\n",
    "                self.val_losses.append(val_loss)\n",
    "\n",
    "                print(f\"Val Loss: {val_loss:.4f}, RMSE: {rmse:.2f} W/m², MAE: {mae:.2f} W/m²\")\n",
    "\n",
    "                # Learning rate scheduling\n",
    "                self.scheduler.step(val_loss)\n",
    "\n",
    "                # Early stopping and model saving\n",
    "                if val_loss < self.best_val_loss:\n",
    "                    self.best_val_loss = val_loss\n",
    "                    self.patience_counter = 0\n",
    "\n",
    "                    # Save best model\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'val_loss': val_loss,\n",
    "                        'config': self.config\n",
    "                    }, os.path.join(self.config['save_dir'], 'best_cnn_model.pth'))\n",
    "\n",
    "                    print(f\"New best model saved! RMSE: {rmse:.2f} W/m²\")\n",
    "                else:\n",
    "                    self.patience_counter += 1\n",
    "\n",
    "                if self.patience_counter >= self.config['early_stopping_patience']:\n",
    "                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                    break\n",
    "\n",
    "        # Save final model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses\n",
    "        }, os.path.join(self.config['save_dir'], 'final_cnn_model.pth'))\n",
    "\n",
    "        # Save training history\n",
    "        history = {\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'config': self.config\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(self.config['log_dir'], 'cnn_training_history.json'), 'w') as f:\n",
    "            json.dump(history, f, indent=2)\n",
    "\n",
    "        print(\"Training completed!\")\n",
    "        return self.train_losses, self.val_losses\n",
    "\n",
    "    def load_model(self, checkpoint_path):\n",
    "        \"\"\"Load a saved model\"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        return checkpoint\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        \"\"\"Evaluate model on test set\"\"\"\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, batch_targets in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "                images, batch_targets = images.to(self.device), batch_targets.to(self.device)\n",
    "\n",
    "                outputs = self.model(images)\n",
    "                predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "                targets.extend(batch_targets.cpu().numpy())\n",
    "\n",
    "        predictions = np.array(predictions)\n",
    "        targets = np.array(targets)\n",
    "\n",
    "        mse = mean_squared_error(targets, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(targets, predictions)\n",
    "\n",
    "        print(f\"Test Results - RMSE: {rmse:.2f} W/m², MAE: {mae:.2f} W/m²\")\n",
    "\n",
    "        return rmse, mae, predictions, targets\n",
    "\n",
    "def train_cnn_nowcasting():\n",
    "    \"\"\"Main function to train CNN nowcasting model\"\"\"\n",
    "\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'learning_rate': 1e-4,\n",
    "        'batch_size': 32,\n",
    "        'num_epochs': 50,\n",
    "        # List of days\n",
    "        'image_dirs': [\n",
    "            '/content/drive/MyDrive/data/processed/2019_01_15',\n",
    "            '/content/drive/MyDrive/data/processed/2019_01_16',\n",
    "            '/content/drive/MyDrive/data/processed/2019_01_17',\n",
    "            '/content/drive/MyDrive/data/processed/2019_01_18',\n",
    "            '/content/drive/MyDrive/data/processed/2019_01_19',\n",
    "            '/content/drive/MyDrive/data/processed/2019_01_20'\n",
    "        ],\n",
    "        'irradiance_files': [\n",
    "            '/content/drive/MyDrive/GIRASOL_DATASET/2019_01_15/pyranometer/2019_01_15.csv',\n",
    "            '/content/drive/MyDrive/GIRASOL_DATASET/2019_01_16/pyranometer/2019_01_16.csv',\n",
    "            '/content/drive/MyDrive/GIRASOL_DATASET/2019_01_17/pyranometer/2019_01_17.csv',\n",
    "            '/content/drive/MyDrive/GIRASOL_DATASET/2019_01_18/pyranometer/2019_01_18.csv',\n",
    "            '/content/drive/MyDrive/GIRASOL_DATASET/2019_01_19/pyranometer/2019_01_19.csv',\n",
    "            '/content/drive/MyDrive/GIRASOL_DATASET/2019_01_20/pyranometer/2019_01_20.csv'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create multi-day dataset\n",
    "    print(\"Loading multi-day dataset...\")\n",
    "    dataset = get_multi_day_dataset(config['image_dirs'], config['irradiance_files'])\n",
    "\n",
    "    # Split dataset\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=2 # Reduced number of workers\n",
    "        )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=2 # Reduced number of workers\n",
    "    )\n",
    "\n",
    "    print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = CNNTrainer(model_type='standard', config=config)\n",
    "\n",
    "    # Train model\n",
    "    train_losses, val_losses = trainer.train(train_loader, val_loader)\n",
    "\n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title('CNN Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('logs/cnn_training_curves.png')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"CNN nowcasting training completed!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_cnn_nowcasting()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
