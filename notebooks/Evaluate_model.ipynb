{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e592f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Ensure the project root and scripts/ are on sys.path so notebook imports work\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.insert(0, repo_root)\n",
    "\n",
    "scripts_path = os.path.join(os.getcwd(), 'scripts')\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.insert(0, scripts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e06ef17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Preparing CNN test dataset...\n",
      "\n",
      "=== Evaluating CNN Nowcasting Model ===\n",
      "CNN Results:\n",
      "  RMSE: 20.35 W/m²\n",
      "  MAE: 16.71 W/m²\n",
      "  R²: -4.5798\n",
      "  Samples: 1679\n",
      "Preparing LSTM test dataset...\n",
      "\n",
      "=== Evaluating LSTM Forecasting Model ===\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy._core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy._core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 491\u001b[39m\n\u001b[32m    487\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m- evaluation_plots/ (visualization plots)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m     \u001b[43mrun_comprehensive_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 463\u001b[39m, in \u001b[36mrun_comprehensive_evaluation\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPreparing LSTM test dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    458\u001b[39m     lstm_test_dataset = SolarTimeSeriesDataset(\n\u001b[32m    459\u001b[39m         irradiance_file=test_config[\u001b[33m'\u001b[39m\u001b[33mirradiance_file\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    460\u001b[39m         sequence_length=test_config[\u001b[33m'\u001b[39m\u001b[33msequence_length\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    461\u001b[39m         forecast_horizon=test_config[\u001b[33m'\u001b[39m\u001b[33mforecast_horizon\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    462\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlstm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_test_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;66;03m# Evaluate Hybrid if available\u001b[39;00m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (os.path.exists(model_paths[\u001b[33m'\u001b[39m\u001b[33mhybrid\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m \n\u001b[32m    467\u001b[39m     os.path.exists(test_config[\u001b[33m'\u001b[39m\u001b[33mimage_dir\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m \n\u001b[32m    468\u001b[39m     os.path.exists(test_config[\u001b[33m'\u001b[39m\u001b[33mirradiance_file\u001b[39m\u001b[33m'\u001b[39m])):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mModelEvaluator.evaluate_lstm\u001b[39m\u001b[34m(self, model_path, test_dataset)\u001b[39m\n\u001b[32m     88\u001b[39m config = checkpoint.get(\u001b[33m'\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m model, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mSolarLSTMForecasting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlstm_hidden_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlstm_num_layers\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mforecast_horizon\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Create test loader\u001b[39;00m\n\u001b[32m    101\u001b[39m test_loader = DataLoader(test_dataset, batch_size=\u001b[32m32\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers=\u001b[32m4\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mModelEvaluator.load_model\u001b[39m\u001b[34m(self, model_class, checkpoint_path, **kwargs)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load a trained model from checkpoint\"\"\"\u001b[39;00m\n\u001b[32m     28\u001b[39m model = model_class(**kwargs).to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m model.load_state_dict(checkpoint[\u001b[33m'\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     31\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\Hybrid CNN-LSTM for Solar Irradiance Forecasting\\venv\\Lib\\site-packages\\torch\\serialization.py:1524\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1516\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1517\u001b[39m                     opened_zipfile,\n\u001b[32m   1518\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1521\u001b[39m                     **pickle_load_args,\n\u001b[32m   1522\u001b[39m                 )\n\u001b[32m   1523\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1524\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1525\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1526\u001b[39m             opened_zipfile,\n\u001b[32m   1527\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1530\u001b[39m             **pickle_load_args,\n\u001b[32m   1531\u001b[39m         )\n\u001b[32m   1532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy._core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy._core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from scripts.cnn_model import SolarCNNRegression, SolarCNNWithFeatureExtraction\n",
    "from scripts.lstm_model import SolarLSTMForecasting, HybridCNNLSTM\n",
    "from scripts.solar_datasets import SolarIrradianceDataset, SolarTimeSeriesDataset, SolarSequenceDataset\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation class for all models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device=None):\n",
    "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.results = {}\n",
    "\n",
    "    def load_model(self, model_class, checkpoint_path, **kwargs):\n",
    "        \"\"\"Load a trained model from checkpoint\"\"\"\n",
    "        model = model_class(**kwargs).to(self.device)\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        return model, checkpoint\n",
    "\n",
    "    def evaluate_cnn(self, model_path, test_dataset):\n",
    "        \"\"\"Evaluate CNN nowcasting model\"\"\"\n",
    "        print(\"\\n=== Evaluating CNN Nowcasting Model ===\")\n",
    "\n",
    "        # Load model\n",
    "        model, checkpoint = self.load_model(SolarCNNRegression, model_path)\n",
    "\n",
    "        # Create test loader\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "        predictions = []\n",
    "        targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, batch_targets in test_loader:\n",
    "                images, batch_targets = images.to(self.device), batch_targets.to(self.device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "                targets.extend(batch_targets.cpu().numpy())\n",
    "\n",
    "        predictions = np.array(predictions)\n",
    "        targets = np.array(targets)\n",
    "\n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(targets, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(targets, predictions)\n",
    "        r2 = r2_score(targets, predictions)\n",
    "\n",
    "        # Store results\n",
    "        self.results['cnn'] = {\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'predictions': predictions.tolist(),\n",
    "            'targets': targets.tolist(),\n",
    "            'num_samples': len(predictions)\n",
    "        }\n",
    "\n",
    "        print(f\"CNN Results:\")\n",
    "        print(f\"  RMSE: {rmse:.2f} W/m²\")\n",
    "        print(f\"  MAE: {mae:.2f} W/m²\")\n",
    "        print(f\"  R²: {r2:.4f}\")\n",
    "        print(f\"  Samples: {len(predictions)}\")\n",
    "\n",
    "        return rmse, mae, r2, predictions, targets\n",
    "\n",
    "    def evaluate_lstm(self, model_path, test_dataset):\n",
    "        \"\"\"Evaluate LSTM forecasting model\"\"\"\n",
    "        print(\"\\n=== Evaluating LSTM Forecasting Model ===\")\n",
    "\n",
    "        # Load checkpoint to get config\n",
    "        checkpoint = torch.load(model_path, map_location=self.device,weights_only=False)\n",
    "        config = checkpoint.get('config', {})\n",
    "\n",
    "        # Load model\n",
    "        model, _ = self.load_model(\n",
    "            SolarLSTMForecasting, \n",
    "            model_path,\n",
    "            input_size=1,\n",
    "            hidden_size=config.get('lstm_hidden_size', 128),\n",
    "            num_layers=config.get('lstm_num_layers', 2),\n",
    "            output_size=config.get('forecast_horizon', 4)\n",
    "        )\n",
    "\n",
    "        # Create test loader\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sequences, targets in test_loader:\n",
    "                sequences, targets = sequences.to(self.device), targets.to(self.device)\n",
    "\n",
    "                outputs = model(sequences)\n",
    "                all_predictions.append(outputs.cpu().numpy())\n",
    "                all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "        predictions = np.concatenate(all_predictions, axis=0)\n",
    "        targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "        # Calculate overall metrics\n",
    "        overall_rmse = np.sqrt(mean_squared_error(targets.flatten(), predictions.flatten()))\n",
    "        overall_mae = mean_absolute_error(targets.flatten(), predictions.flatten())\n",
    "        overall_r2 = r2_score(targets.flatten(), predictions.flatten())\n",
    "\n",
    "        # Calculate per-step metrics\n",
    "        forecast_horizon = predictions.shape[1]\n",
    "        step_metrics = []\n",
    "\n",
    "        for step in range(forecast_horizon):\n",
    "            step_rmse = np.sqrt(mean_squared_error(targets[:, step], predictions[:, step]))\n",
    "            step_mae = mean_absolute_error(targets[:, step], predictions[:, step])\n",
    "            step_r2 = r2_score(targets[:, step], predictions[:, step])\n",
    "\n",
    "            step_metrics.append({\n",
    "                'step': step + 1,\n",
    "                'rmse': step_rmse,\n",
    "                'mae': step_mae,\n",
    "                'r2': step_r2\n",
    "            })\n",
    "\n",
    "        # Store results\n",
    "        self.results['lstm'] = {\n",
    "            'overall_rmse': overall_rmse,\n",
    "            'overall_mae': overall_mae,\n",
    "            'overall_r2': overall_r2,\n",
    "            'step_metrics': step_metrics,\n",
    "            'predictions': predictions.tolist(),\n",
    "            'targets': targets.tolist(),\n",
    "            'num_samples': len(predictions)\n",
    "        }\n",
    "\n",
    "        print(f\"LSTM Results:\")\n",
    "        print(f\"  Overall RMSE: {overall_rmse:.2f} W/m²\")\n",
    "        print(f\"  Overall MAE: {overall_mae:.2f} W/m²\")\n",
    "        print(f\"  Overall R²: {overall_r2:.4f}\")\n",
    "        print(f\"  Per-step RMSE: {[f'{m['rmse']:.2f}' for m in step_metrics]}\")\n",
    "        print(f\"  Samples: {len(predictions)}\")\n",
    "\n",
    "        return overall_rmse, overall_mae, overall_r2, step_metrics, predictions, targets\n",
    "\n",
    "    def evaluate_hybrid(self, model_path, test_dataset):\n",
    "        \"\"\"Evaluate hybrid CNN-LSTM model\"\"\"\n",
    "        print(\"\\n=== Evaluating Hybrid CNN-LSTM Model ===\")\n",
    "\n",
    "        # Load checkpoint to get config\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        config = checkpoint.get('config', {})\n",
    "\n",
    "        # Load model\n",
    "        model, _ = self.load_model(\n",
    "            HybridCNNLSTM,\n",
    "            model_path,\n",
    "            sequence_length=config.get('sequence_length', 20),\n",
    "            lstm_hidden_size=config.get('lstm_hidden_size', 128),\n",
    "            forecast_horizon=config.get('forecast_horizon', 4)\n",
    "        )\n",
    "\n",
    "        # Create test loader\n",
    "        test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "        all_nowcast_pred = []\n",
    "        all_nowcast_target = []\n",
    "        all_forecast_pred = []\n",
    "        all_forecast_target = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for image_sequences, historical_irradiance, future_irradiance in test_loader:\n",
    "                image_sequences = image_sequences.to(self.device)\n",
    "                historical_irradiance = historical_irradiance.to(self.device)\n",
    "                future_irradiance = future_irradiance.to(self.device)\n",
    "\n",
    "                nowcasts, forecasts = model(image_sequences)\n",
    "\n",
    "                all_nowcast_pred.append(nowcasts.squeeze(-1).cpu().numpy())\n",
    "                all_nowcast_target.append(historical_irradiance.squeeze(-1).cpu().numpy())\n",
    "                all_forecast_pred.append(forecasts.cpu().numpy())\n",
    "                all_forecast_target.append(future_irradiance.cpu().numpy())\n",
    "\n",
    "        nowcast_pred = np.concatenate(all_nowcast_pred, axis=0)\n",
    "        nowcast_target = np.concatenate(all_nowcast_target, axis=0)\n",
    "        forecast_pred = np.concatenate(all_forecast_pred, axis=0)\n",
    "        forecast_target = np.concatenate(all_forecast_target, axis=0)\n",
    "\n",
    "        # Nowcast metrics\n",
    "        nowcast_rmse = np.sqrt(mean_squared_error(nowcast_target.flatten(), nowcast_pred.flatten()))\n",
    "        nowcast_mae = mean_absolute_error(nowcast_target.flatten(), nowcast_pred.flatten())\n",
    "        nowcast_r2 = r2_score(nowcast_target.flatten(), nowcast_pred.flatten())\n",
    "\n",
    "        # Forecast metrics\n",
    "        forecast_rmse = np.sqrt(mean_squared_error(forecast_target.flatten(), forecast_pred.flatten()))\n",
    "        forecast_mae = mean_absolute_error(forecast_target.flatten(), forecast_pred.flatten())\n",
    "        forecast_r2 = r2_score(forecast_target.flatten(), forecast_pred.flatten())\n",
    "\n",
    "        # Per-step forecast metrics\n",
    "        forecast_horizon = forecast_pred.shape[1]\n",
    "        forecast_step_metrics = []\n",
    "\n",
    "        for step in range(forecast_horizon):\n",
    "            step_rmse = np.sqrt(mean_squared_error(forecast_target[:, step], forecast_pred[:, step]))\n",
    "            step_mae = mean_absolute_error(forecast_target[:, step], forecast_pred[:, step])\n",
    "            step_r2 = r2_score(forecast_target[:, step], forecast_pred[:, step])\n",
    "\n",
    "            forecast_step_metrics.append({\n",
    "                'step': step + 1,\n",
    "                'rmse': step_rmse,\n",
    "                'mae': step_mae,\n",
    "                'r2': step_r2\n",
    "            })\n",
    "\n",
    "        # Store results\n",
    "        self.results['hybrid'] = {\n",
    "            'nowcast_rmse': nowcast_rmse,\n",
    "            'nowcast_mae': nowcast_mae,\n",
    "            'nowcast_r2': nowcast_r2,\n",
    "            'forecast_rmse': forecast_rmse,\n",
    "            'forecast_mae': forecast_mae,\n",
    "            'forecast_r2': forecast_r2,\n",
    "            'forecast_step_metrics': forecast_step_metrics,\n",
    "            'num_samples': len(forecast_pred)\n",
    "        }\n",
    "\n",
    "        print(f\"Hybrid Results:\")\n",
    "        print(f\"  Nowcast RMSE: {nowcast_rmse:.2f} W/m²\")\n",
    "        print(f\"  Nowcast MAE: {nowcast_mae:.2f} W/m²\")\n",
    "        print(f\"  Nowcast R²: {nowcast_r2:.4f}\")\n",
    "        print(f\"  Forecast RMSE: {forecast_rmse:.2f} W/m²\")\n",
    "        print(f\"  Forecast MAE: {forecast_mae:.2f} W/m²\")\n",
    "        print(f\"  Forecast R²: {forecast_r2:.4f}\")\n",
    "        print(f\"  Per-step Forecast RMSE: {[f'{m['rmse']:.2f}' for m in forecast_step_metrics]}\")\n",
    "        print(f\"  Samples: {len(forecast_pred)}\")\n",
    "\n",
    "        return (nowcast_rmse, nowcast_mae, forecast_rmse, forecast_mae, \n",
    "                forecast_step_metrics, nowcast_pred, forecast_pred)\n",
    "\n",
    "    def plot_results(self, save_dir='evaluation_plots'):\n",
    "        \"\"\"Create visualization plots\"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # CNN scatter plot\n",
    "        if 'cnn' in self.results:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "\n",
    "            targets = np.array(self.results['cnn']['targets'])\n",
    "            predictions = np.array(self.results['cnn']['predictions'])\n",
    "\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.scatter(targets, predictions, alpha=0.6, s=20)\n",
    "            plt.plot([targets.min(), targets.max()], [targets.min(), targets.max()], 'r--', lw=2)\n",
    "            plt.xlabel('Actual Irradiance (W/m²)')\n",
    "            plt.ylabel('Predicted Irradiance (W/m²)')\n",
    "            plt.title(f'CNN Nowcasting (RMSE: {self.results[\"cnn\"][\"rmse\"]:.2f} W/m²)')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            # Residuals plot\n",
    "            plt.subplot(2, 2, 2)\n",
    "            residuals = predictions - targets\n",
    "            plt.scatter(targets, residuals, alpha=0.6, s=20)\n",
    "            plt.axhline(y=0, color='r', linestyle='--')\n",
    "            plt.xlabel('Actual Irradiance (W/m²)')\n",
    "            plt.ylabel('Residuals (W/m²)')\n",
    "            plt.title('CNN Residuals')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{save_dir}/cnn_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "        # LSTM step-wise performance\n",
    "        if 'lstm' in self.results:\n",
    "            step_metrics = self.results['lstm']['step_metrics']\n",
    "            steps = [m['step'] for m in step_metrics]\n",
    "            rmse_values = [m['rmse'] for m in step_metrics]\n",
    "            mae_values = [m['mae'] for m in step_metrics]\n",
    "\n",
    "            plt.figure(figsize=(12, 6))\n",
    "\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.bar(steps, rmse_values, color='skyblue', alpha=0.7)\n",
    "            plt.xlabel('Forecast Step')\n",
    "            plt.ylabel('RMSE (W/m²)')\n",
    "            plt.title('LSTM Forecast RMSE by Step')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.bar(steps, mae_values, color='lightcoral', alpha=0.7)\n",
    "            plt.xlabel('Forecast Step')\n",
    "            plt.ylabel('MAE (W/m²)')\n",
    "            plt.title('LSTM Forecast MAE by Step')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{save_dir}/lstm_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "        # Model comparison\n",
    "        if len(self.results) > 1:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "\n",
    "            models = []\n",
    "            rmse_values = []\n",
    "            mae_values = []\n",
    "\n",
    "            if 'cnn' in self.results:\n",
    "                models.append('CNN\\nNowcasting')\n",
    "                rmse_values.append(self.results['cnn']['rmse'])\n",
    "                mae_values.append(self.results['cnn']['mae'])\n",
    "\n",
    "            if 'lstm' in self.results:\n",
    "                models.append('LSTM\\nForecasting')\n",
    "                rmse_values.append(self.results['lstm']['overall_rmse'])\n",
    "                mae_values.append(self.results['lstm']['overall_mae'])\n",
    "\n",
    "            if 'hybrid' in self.results:\n",
    "                models.append('Hybrid\\nNowcast')\n",
    "                rmse_values.append(self.results['hybrid']['nowcast_rmse'])\n",
    "                mae_values.append(self.results['hybrid']['nowcast_mae'])\n",
    "\n",
    "                models.append('Hybrid\\nForecast')\n",
    "                rmse_values.append(self.results['hybrid']['forecast_rmse'])\n",
    "                mae_values.append(self.results['hybrid']['forecast_mae'])\n",
    "\n",
    "            x = np.arange(len(models))\n",
    "            width = 0.35\n",
    "\n",
    "            plt.subplot(1, 1, 1)\n",
    "            plt.bar(x - width/2, rmse_values, width, label='RMSE', color='skyblue', alpha=0.8)\n",
    "            plt.bar(x + width/2, mae_values, width, label='MAE', color='lightcoral', alpha=0.8)\n",
    "\n",
    "            plt.xlabel('Models')\n",
    "            plt.ylabel('Error (W/m²)')\n",
    "            plt.title('Model Performance Comparison')\n",
    "            plt.xticks(x, models)\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            # Add value labels on bars\n",
    "            for i, (rmse, mae) in enumerate(zip(rmse_values, mae_values)):\n",
    "                plt.text(i - width/2, rmse + 1, f'{rmse:.1f}', ha='center', va='bottom')\n",
    "                plt.text(i + width/2, mae + 1, f'{mae:.1f}', ha='center', va='bottom')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{save_dir}/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "        print(f\"\\nPlots saved to {save_dir}/\")\n",
    "\n",
    "    def save_results(self, filename='evaluation_results.json'):\n",
    "        \"\"\"Save evaluation results to JSON\"\"\"\n",
    "        results_with_metadata = {\n",
    "            'evaluation_timestamp': datetime.now().isoformat(),\n",
    "            'device': str(self.device),\n",
    "            'results': self.results\n",
    "        }\n",
    "\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(results_with_metadata, f, indent=2)\n",
    "\n",
    "        print(f\"\\nResults saved to {filename}\")\n",
    "\n",
    "    def generate_report(self, filename='evaluation_report.txt'):\n",
    "        \"\"\"Generate a comprehensive text report\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(\"SOLAR IRRADIANCE FORECASTING - MODEL EVALUATION REPORT\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Device: {self.device}\\n\\n\")\n",
    "\n",
    "            if 'cnn' in self.results:\n",
    "                f.write(\"CNN NOWCASTING MODEL\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "                r = self.results['cnn']\n",
    "                f.write(f\"RMSE: {r['rmse']:.2f} W/m²\\n\")\n",
    "                f.write(f\"MAE: {r['mae']:.2f} W/m²\\n\")\n",
    "                f.write(f\"R²: {r['r2']:.4f}\\n\")\n",
    "                f.write(f\"Samples: {r['num_samples']}\\n\\n\")\n",
    "\n",
    "            if 'lstm' in self.results:\n",
    "                f.write(\"LSTM FORECASTING MODEL\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "                r = self.results['lstm']\n",
    "                f.write(f\"Overall RMSE: {r['overall_rmse']:.2f} W/m²\\n\")\n",
    "                f.write(f\"Overall MAE: {r['overall_mae']:.2f} W/m²\\n\")\n",
    "                f.write(f\"Overall R²: {r['overall_r2']:.4f}\\n\")\n",
    "                f.write(\"Per-step Performance:\\n\")\n",
    "                for m in r['step_metrics']:\n",
    "                    f.write(f\"  Step {m['step']}: RMSE={m['rmse']:.2f}, MAE={m['mae']:.2f}, R²={m['r2']:.4f}\\n\")\n",
    "                f.write(f\"Samples: {r['num_samples']}\\n\\n\")\n",
    "\n",
    "            if 'hybrid' in self.results:\n",
    "                f.write(\"HYBRID CNN-LSTM MODEL\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "                r = self.results['hybrid']\n",
    "                f.write(\"Nowcasting Performance:\\n\")\n",
    "                f.write(f\"  RMSE: {r['nowcast_rmse']:.2f} W/m²\\n\")\n",
    "                f.write(f\"  MAE: {r['nowcast_mae']:.2f} W/m²\\n\")\n",
    "                f.write(f\"  R²: {r['nowcast_r2']:.4f}\\n\")\n",
    "                f.write(\"Forecasting Performance:\\n\")\n",
    "                f.write(f\"  RMSE: {r['forecast_rmse']:.2f} W/m²\\n\")\n",
    "                f.write(f\"  MAE: {r['forecast_mae']:.2f} W/m²\\n\")\n",
    "                f.write(f\"  R²: {r['forecast_r2']:.4f}\\n\")\n",
    "                f.write(\"Per-step Forecast Performance:\\n\")\n",
    "                for m in r['forecast_step_metrics']:\n",
    "                    f.write(f\"  Step {m['step']}: RMSE={m['rmse']:.2f}, MAE={m['mae']:.2f}, R²={m['r2']:.4f}\\n\")\n",
    "                f.write(f\"Samples: {r['num_samples']}\\n\\n\")\n",
    "\n",
    "        print(f\"Report saved to {filename}\")\n",
    "\n",
    "\n",
    "def run_comprehensive_evaluation():\n",
    "    \"\"\"Run evaluation on all available models\"\"\"\n",
    "    evaluator = ModelEvaluator()\n",
    "\n",
    "    # Paths to trained models\n",
    "    model_paths = {\n",
    "        'cnn': r'D:\\Projects\\Hybrid CNN-LSTM for Solar Irradiance Forecasting\\models\\best_cnn_model.pth',\n",
    "        'lstm': r'D:\\Projects\\Hybrid CNN-LSTM for Solar Irradiance Forecasting\\models/best_lstm_model.pth',\n",
    "        'hybrid': r'D:\\Projects\\Hybrid CNN-LSTM for Solar Irradiance Forecasting\\models/best_hybrid_model.pth'\n",
    "    }\n",
    "\n",
    "    # Test data configuration\n",
    "    test_config = {\n",
    "        'image_dir': r'D:\\Projects\\Hybrid CNN-LSTM for Solar Irradiance Forecasting\\data\\test\\infrared',  # Adjust path as needed\n",
    "        'irradiance_file': r'D:\\Projects\\Hybrid CNN-LSTM for Solar Irradiance Forecasting\\data\\test\\pyranometer\\2019_01_15.csv',  # Adjust path as needed\n",
    "        'sequence_length': 20,\n",
    "        'forecast_horizon': 4\n",
    "    }\n",
    "\n",
    "    # Evaluate CNN if available\n",
    "    if os.path.exists(model_paths['cnn']) and os.path.exists(test_config['image_dir']):\n",
    "        print(\"Preparing CNN test dataset...\")\n",
    "        cnn_test_dataset = SolarIrradianceDataset(\n",
    "            image_dir=test_config['image_dir'],\n",
    "            irradiance_file=test_config['irradiance_file']\n",
    "        )\n",
    "        evaluator.evaluate_cnn(model_paths['cnn'], cnn_test_dataset)\n",
    "\n",
    "    # Evaluate LSTM if available\n",
    "    if os.path.exists(model_paths['lstm']) and os.path.exists(test_config['irradiance_file']):\n",
    "        print(\"Preparing LSTM test dataset...\")\n",
    "        lstm_test_dataset = SolarTimeSeriesDataset(\n",
    "            irradiance_file=test_config['irradiance_file'],\n",
    "            sequence_length=test_config['sequence_length'],\n",
    "            forecast_horizon=test_config['forecast_horizon']\n",
    "        )\n",
    "        evaluator.evaluate_lstm(model_paths['lstm'], lstm_test_dataset)\n",
    "\n",
    "    # Evaluate Hybrid if available\n",
    "    if (os.path.exists(model_paths['hybrid']) and \n",
    "        os.path.exists(test_config['image_dir']) and \n",
    "        os.path.exists(test_config['irradiance_file'])):\n",
    "        print(\"Preparing Hybrid test dataset...\")\n",
    "        hybrid_test_dataset = SolarSequenceDataset(\n",
    "            image_dir=test_config['image_dir'],\n",
    "            irradiance_file=test_config['irradiance_file'],\n",
    "            sequence_length=test_config['sequence_length'],\n",
    "            forecast_horizon=test_config['forecast_horizon']\n",
    "        )\n",
    "        evaluator.evaluate_hybrid(model_paths['hybrid'], hybrid_test_dataset)\n",
    "\n",
    "    # Generate outputs\n",
    "    evaluator.plot_results()\n",
    "    evaluator.save_results()\n",
    "    evaluator.generate_report()\n",
    "\n",
    "    print(\"\\n=== Evaluation Complete ===\")\n",
    "    print(\"Check the following files:\")\n",
    "    print(\"- evaluation_results.json (detailed metrics)\")\n",
    "    print(\"- evaluation_report.txt (summary report)\")\n",
    "    print(\"- evaluation_plots/ (visualization plots)\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_comprehensive_evaluation()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
